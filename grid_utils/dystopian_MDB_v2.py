#!/slowfs/dcopt105/vasquez/cnda/Conda/bin/python
 
import pickle, os, re
import datetime, subprocess
import pandas as pd
from pandas import ExcelWriter
import xml.etree.ElementTree as et
import datetime
from balancer import *
from datetime import datetime
# Mongo db
from pymongo import MongoClient
from dump_csv import draw_summary


import pprint
pp = pprint.PrettyPrinter(indent = 1, depth= 3)

# global paths to qsta utilities
# just in case
qstat  = '/remote/sge3/default/bin/lx-amd64/qstat'
qalter = '/remote/sge3/default/bin/lx-amd64/qalter'
# available_info = ['JB_job_number', 'JAT_prio', 'JAT_ntix', 'JB_nurg', 'JB_urg', 'JB_rrcontr', 'JB_wtcontr', 'JB_dlcontr', 'JB_name', 'JB_owner', 'JB_project', 'JB_department', 'state', 'JB_submission_time', 'tickets', 'JB_override_tickets', 'JB_jobshare', 'otickets', 'ftickets', 'stickets', 'JAT_share', 'queue_name', 'jclass_name', 'slots']


def get_qstat(user, farm):
    farms = {
        'gala' : 'source /remote/sge/default/galapagos/common/settings.csh; set qstat_real = qstat',
        'snps' : 'source /remote/sge/default/snps/common/settings.csh; set qstat_real = qstat',
    }

    cmd = '%s; qstat -u %s -r -xml'%(farms[farm],user)
    cmd_obj = subprocess.run(cmd, executable= '/bin/csh',shell = True,stdout=subprocess.PIPE)    
    cmd_ret = cmd_obj.stdout.decode("utf-8")
    return cmd_ret

def qstat_from_xml(xml):

    # parse the xml file from a synopsys grid like
    # from a textfile generated by
    # qstat -u <some_user> -xml > some_xml_file.xml
    # 
    # returns a dict in the form:
    # {running : [{jb1_info1: ... , jb1_info2: ...,},{jb2_info1:..., jb2_info2:...}...],
    #  pending : [{jba_info1: ... , jba_info2: ...,},{jbb_info1:..., jbb_info2:...}...],}            
    # 
    # -vasquez- 

    if type(xml) == str:
        all_jobs = et.fromstring(xml)
    else:
        # parse the xml file
        tree = et.parse(xml)
        all_jobs = tree.getroot()

    # The xml output structure for qstat has two queues
    # one with all running jobs, and another one with
    # the ending ones,
    # 
    # queue_info
    #     job_list state: running
    #         job_number 
    #         prior
    #         ...
    #     job_list state: running
    #         job_number 
    #         prior
    #         ...
    #     ...
    # job_info
    #     job_list state: pending
    #         job_number
    #         prior
    #         ...
    #     job_list state: pending
    #         ...
    #     ...
    # 
    # queue_info: contains only running jobs
    # job_info: contains only pending jobs

    qstat_dict = {}

    for queue in all_jobs:

        if queue.tag == 'queue_info':
            current_queue = 'running'
        elif queue.tag == 'job_info':
            current_queue = 'pending'
        else:
            print('Warning: we are not prepared for fields like: ' + str(queue.tag) + ' in the top level') 

        if current_queue not in qstat_dict:        
            qstat_dict[current_queue] = []

        for job_info in queue:
            
            # the job info will always be running for queue_info
            # and pending for job_info, ...i hope so 
            
            job_info_dict = {}

            for field in job_info:

                tag = field.tag    
                value = field.text
                attrib = field.attrib

                job_info_dict['%s/%s'%(tag,attrib)] = value

            
            qstat_dict[current_queue].append(job_info_dict)
                
    # pp.pprint(qstat_dict)
    return qstat_dict

def sum_by(qstat_dict):

    excluded = 'full_job_name arch queue_name jclass_name health binding'.split()
    index = 1
    machines = {}
    usrs = {}

    numbers = {}
    numbers['running'] = {'total': len(qstat_dict['running'])}
    numbers['pending'] = {'total': len(qstat_dict['pending'])}

    numbers['running']['reg'] = 0
    numbers['running']['ext'] = 0

    numbers['pending']['reg'] = 0
    numbers['pending']['ext'] = 0

    cel_names = []


    for status in qstat_dict:
        # qstat_dict has two queues, running and pending
        for job in qstat_dict[status]:
            for data in job.keys():
                cel_name = ''
                # lets filter tricky nested names like this
                # hard_request/{'name': 'hconfig', 'resource_contribution': '0.000000'}
                ptrn_hard_req = r'hard_request\/\{\'name\'\:\s\'(.+)\',.+'

                # JB_job_number/{}
                ptrn_key = r'(.+)\/\{\}'

                m_hard_req = re.match(ptrn_hard_req, data)
                m_key      = re.match(ptrn_key, data)

                if m_hard_req: cel_name = m_hard_req.group(1)
                elif m_key: cel_name = m_key.group(1)
                else: cel_name = data

                if cel_name not in excluded:
    
                    # accounting
                    # machines[status]['no_rt'] = []
                    if cel_name == 'hconfig':
                        machine_name = job[data] if len(job[data])>3 else 'no_rt'
                        
                        if machine_name not in machines:
                            machines[machine_name]= {}
                            machines[machine_name][status] =  1
                        else:
                            if status not in machines[machine_name]:
                                machines[machine_name][status] = 0
                            machines[machine_name][status] += 1 

                    if cel_name == 'JB_owner':
                        user_name = job[data]
                        if user_name not in usrs:
                            usrs[user_name] = {}
                            usrs[user_name][status] = 1
                        else:
                            if status not in usrs[user_name]: 
                                usrs[user_name][status] = 0
                            usrs[user_name][status] += 1

                    if cel_name == 'JB_name':
                        name = job[data]
                        f_name = name.split('%')[0] if '%' in name else name

                        if 'ex' in f_name:
                            numbers[status]['ext'] += 1
                        else:
                            numbers[status]['reg'] += 1

    numbers['pend_run_ratio'] = numbers['pending']['total'] / numbers['running']['total'] if numbers['running']['total'] else None
    numbers['pend_run_ext_ratio'] = numbers['pending']['ext'] / numbers['running']['ext'] if numbers['running']['ext'] else None
    numbers['pend_run_reg_ratio'] = numbers['pending']['reg'] / numbers['running']['reg'] if numbers['running']['reg'] else None
    numbers['ext_reg_pend_ratio'] = numbers['pending']['ext'] / numbers['pending']['reg'] if numbers['pending']['reg'] else None
    numbers['ext_reg_run_ratio'] = numbers['running']['ext'] / numbers['running']['reg'] if numbers['running']['reg'] else None
                        
    #pp.pprint(usrs)
    #pp.pprint(machines)
    #pp.pprint(numbers)
    return usrs, machines, numbers





if os.path.exists('collecting_farm_status.touch'):
    print('there is a another instance of the collector running')
    exit()
else:
    open('collecting_farm_status.touch', 'w')

####################################################################

farm = 'gala'
le_stamp = datetime.now()

users    = 'dcntqor6 dcntqor7 rmorale vasquez omars andresp'
excluded = 'full_job_name arch queue_name jclass_name health binding'.split()


###################################33
# data extraction

pp.pprint('Getting Qstat data...')
xml = get_qstat(users,'gala')
qstat_dict = qstat_from_xml(xml)
user_sum, machine_sum, type_total_sum = sum_by(qstat_dict)

# pp.pprint(qstat_dict)
####################################3
# the function def could be start here

# # Mongo Stuff
client    = MongoClient('pvdc002', 27017)
dc_db     = client.dcnxt_data
farm_data = dc_db['farm']

save_headers = '''
job_number
job_state
job_name
flow
design
priority
owner
submission_time
start_time
hconfig
'''.strip().split('\n')

P = len(qstat_dict['pending'])
R = len(qstat_dict['running'])
T = P + R

print('T: %s   R: %s   P: %s'%(T,R,P))

snapshot = {
    'farm': farm,
    'timestamp': le_stamp, 
    'users' : users.split(),
    'jobs_data': [],
    'users_summary': user_sum,
    'hconfig_summary' : machine_sum,
    'extra_reg_summary': type_total_sum,
    'inflow': 0.0,
    'outflow': 0.0,
    'avg_inflow': 0.0,
    'avg_outflow': 0.0
}

print(le_stamp)
print('Gathering job status...')

for name, Q in qstat_dict.items():
    for J in Q:
        job_num = J['JB_job_number/{}']        
        
        # pp.pprint(J)

        jb_dict = {
            'job_number'     : J['JB_job_number/{}'],
            'job_state'      : J['state/{}'],
            'job_name'       : J['JB_name/{}'],
            'flow'           : J['JB_name/{}'].split('%')[0] if '%' in J['JB_name/{}'] else None,
            'design'         : J['JB_name/{}'].split('%')[1] if '%' in J['JB_name/{}'] else None,
            'priority'       : J['JAT_prio/{}'],
            'owner'          : J['JB_owner/{}'],
            'submission_time': J['JB_submission_time/{}'] if 'JB_submission_time/{}' in J else None,
            'start_time'     : J['JAT_start_time/{}'] if 'JAT_start_time/{}' in J else None,
            'hconfig'        : J["hard_request/{'name': 'hconfig', 'resource_contribution': '0.000000'}"] if "hard_request/{'name': 'hconfig', 'resource_contribution': '0.000000'}" in J else None
        }

        snapshot['jobs_data'].append(jb_dict.copy())    

####################################################################
# get the latest data to compute the input and outpout job flows
print('Computing input and output flows')
snapshots_all = sorted(farm_data.find(), key=lambda i:i['timestamp'])
prev_snapshot = snapshots_all[-1]

# get plain list of jobs
snapshot_job_ls      = [j['job_number'] for j in snapshot['jobs_data']]
prev_snapshot_job_ls = [j['job_number'] for j in prev_snapshot['jobs_data']]

# count the new jobs
input_job_count  = 0
for job in snapshot_job_ls:
    if job not in prev_snapshot_job_ls:
        input_job_count += 1

output_job_count  = 0
for job in prev_snapshot_job_ls:
    if job not in snapshot_job_ls:
        output_job_count += 1

# calculate time between two snapshots
time_diff_sec = (snapshot['timestamp'] - prev_snapshot['timestamp']).total_seconds()
time_diff_hour = time_diff_sec / 3600

print(input_job_count, output_job_count, time_diff_hour)
# calculate the flows
snapshot['inflow']  = input_job_count  / time_diff_hour
snapshot['outflow'] = output_job_count / time_diff_hour
print(snapshot['inflow'],snapshot['outflow'])


# get an average of inflow and outflow
avg_win = 20
latest_inflows = []
latest_outflows = []
print('Computing average flows for a %sX window'%str(avg_win))
for snp in farm_data.find():
    if 'inflow' in snp and (len(latest_inflows) < avg_win):
        latest_inflows.append(snp['inflow'])
    if 'outflow' in snp and (len(latest_outflows) < avg_win):
        latest_outflows.append(snp['outflow'])

snapshot['avg_inflow'] = (sum(latest_inflows) + snapshot['inflow']) / (len(latest_inflows)+1)
snapshot['avg_outflow'] = (sum(latest_outflows)+ snapshot['outflow']) /(len(latest_outflows)+1)


print('submitting data to DB...')
farm_data.insert_one(snapshot)


draw_summary()

if os.path.exists('collecting_farm_status.touch'):
    os.remove('collecting_farm_status.touch')

